# Evaluating Linux EDR Syscall Visibility (baseline vs io_uring)

## Project Context
This project evaluates whether a Linux EDR that relies on syscall-level telemetry (Wazuh + auditd) maintains visibility when equivalent activity is performed via an alternative execution pathway (io_uring). The goal is to measure detection coverage and highlight potential telemetry gaps, then provide defensive recommendations.

## Architecture (Lab Setup)
This work is performed in an isolated lab environment using three VMs:

- **Attacker VM (Ubuntu):** launches the test workloads and automation scripts.
- **Target VM (RHEL):** runs Wazuh agent and auditd to collect telemetry during each run.
- **Wazuh Manager VM:** centralizes alerts/logs from the target.

High-level flow:
1) Workload runs on **Target VM**
2) **auditd** + **Wazuh agent** collect events
3) Events/alerts are sent to the **Wazuh Manager**
4) Logs are collected and processed into **derived CSV metrics** for analysis

## What This Repository Contains
- `scripts/` — run, collect, and process scripts to reproduce experiments
- `data/processed/` — derived CSV metrics (tracked in Git)
- `data/raw/` — optional raw logs (NOT tracked; kept local)
- `analysis/` — notebook used to analyze CSV metrics
- `environment/` — setup notes and version pinning
- `DATA_README.md` — documents data sources, parameters, and output schema
- `ETHICS.md` — ethical + safety considerations

## Dependencies
Target VM (RHEL):
- gcc, make (for compiling workloads)
- auditd
- Wazuh agent

Manager VM:
- Wazuh manager (OVA or installed)

Analysis machine (Attacker VM or your local machine):
- Python 3.10+
- pandas
- matplotlib

Install Python dependencies:
```bash
pip install -r requirements.txt
```
## Reproducibility Workflow (Run → Collect → Process → Analyze)
0) One-time Environment Setup

See environment/setup.md for VM configuration, versions, and log paths.

1) Run baseline workload (traditional syscalls)
```bash
bash scripts/run_baseline.sh
```
2) Run io_uring workload (alternative execution path)
```bash
bash scripts/run_io_uring.sh
```
3) Collect logs and artifacts
```bash
bash scripts/collect_logs.sh <RUN_ID>
```
4) Process logs into derived metrics
```
python scripts/process_logs.py --run-id <RUN_ID>
```
5) Analyze results

Open and execute:

analysis/analysis.ipynb

Validation: Confirming io_uring Activity vs Syscall Visibility

Each experiment run produces a ground-truth artifact (e.g., a file written or expected program output) to confirm that the workload executed successfully. Syscall visibility is validated by comparing:

auditd and Wazuh syscall event coverage for baseline versus io_uring runs, and

an optional lightweight kernel tracing sanity check to confirm io_uring activity occurred while traditional syscall visibility is reduced.

## Output Files

Derived outputs generated by the workflow:

data/processed/run_<RUN_ID>_metrics.csv

data/processed/run_<RUN_ID>_metadata.json

Raw logs (optional, local-only):

data/raw/<RUN_ID>/

See DATA_README.md for output schema and data collection details.

## Notes on Variability

Exact metric values may vary across systems due to kernel scheduling and buffering. Reproducibility is defined by consistent workflow execution, identical output schemas, and comparable trends under equivalent configurations.

## License

For academic use only. See ETHICS.md.
