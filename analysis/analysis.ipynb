{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# io_uring EDR Evasion Analysis\n",
    "**CSC 786 - Dakota State University**  \n",
    "**Author:** Michael Mendoza\n",
    "\n",
    "---\n",
    "\n",
    "## Research Question\n",
    "Can io_uring operations evade traditional syscall-based EDR monitoring?\n",
    "\n",
    "## Hypothesis\n",
    "File and network operations performed via io_uring's submission queue will not generate the same audit events as equivalent traditional syscall operations, creating a detection blind spot for EDR solutions.\n",
    "\n",
    "## MITRE ATT&CK Relevance\n",
    "| Technique | Description | io_uring Impact |\n",
    "|-----------|-------------|----------------|\n",
    "| T1005 | Data from Local System | File reads via io_uring evade file access logging |\n",
    "| T1071 | Application Layer Protocol | Network connections via io_uring evade socket monitoring |\n",
    "| T1562.001 | Impair Defenses | io_uring bypasses syscall-based detection mechanisms |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clone-repo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repository (for Colab)\n",
    "!git clone https://github.com/liIBits/linux-edr-evasion.git 2>/dev/null || echo \"Repo already exists\"\n",
    "%cd linux-edr-evasion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import shutil\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Style configuration\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams.update({'figure.figsize': (10, 6), 'font.size': 12, 'axes.titlesize': 14, 'axes.labelsize': 12})\n",
    "COLORS = {'traditional': '#2ecc71', 'io_uring': '#e74c3c', 'setup': '#9b59b6'}\n",
    "\n",
    "# Output directories - use the repo's analysis folder structure\n",
    "RESULTS_DIR = Path('analysis/results')\n",
    "FIGURES_DIR = RESULTS_DIR / 'figures'\n",
    "TABLES_DIR = RESULTS_DIR / 'tables'\n",
    "\n",
    "# Clean old figures and create fresh directories\n",
    "if FIGURES_DIR.exists():\n",
    "    shutil.rmtree(FIGURES_DIR)\n",
    "if TABLES_DIR.exists():\n",
    "    shutil.rmtree(TABLES_DIR)\n",
    "\n",
    "FIGURES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "TABLES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f'Figures will be saved to: {FIGURES_DIR.absolute()}')\n",
    "print(f'Tables will be saved to: {TABLES_DIR.absolute()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load-header",
   "metadata": {},
   "source": [
    "## 1. Data Loading & Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find and load the most recent CSV\n",
    "processed_dir = Path('data/processed')\n",
    "if not processed_dir.exists():\n",
    "    raise FileNotFoundError(f'Data directory not found: {processed_dir}')\n",
    "\n",
    "csv_files = sorted(processed_dir.glob('runs_*.csv'))\n",
    "if not csv_files:\n",
    "    raise FileNotFoundError('No runs_*.csv files found')\n",
    "\n",
    "csv_path = csv_files[-1]\n",
    "print(f'Dataset: {csv_path.name}')\n",
    "\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Ensure required columns exist\n",
    "for col, default in [('iouring_hits', 0), ('path_hits', -1)]:\n",
    "    if col not in df.columns: df[col] = default\n",
    "\n",
    "# Classify test cases\n",
    "df['method'] = df['case'].apply(lambda x: 'io_uring' if 'uring' in x.lower() else 'traditional')\n",
    "df['operation'] = df['case'].apply(lambda x:\n",
    "    'File Write' if 'file_io' in x.lower() else\n",
    "    'File Read' if 'read_file' in x.lower() or 'openat' in x.lower() else\n",
    "    'Network' if 'net' in x.lower() else\n",
    "    'Process Exec' if 'exec' in x.lower() else 'Other')\n",
    "\n",
    "# Key metrics\n",
    "df['total_syscall_events'] = df['file_hits'] + df['net_hits'] + df['exec_hits']\n",
    "df['iouring_setup_detected'] = df['iouring_hits'] > 0\n",
    "df['path_visible'] = df['path_hits'] > 0\n",
    "\n",
    "print(f'Total test runs: {len(df)}')\n",
    "print(f'Iterations: {df.iteration.nunique()}')\n",
    "print(f'Test cases: {df[\"case\"].nunique()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overview-header",
   "metadata": {},
   "source": [
    "## 2. Experiment Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "overview",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test case summary\n",
    "summary = df.groupby(['operation', 'method']).agg({\n",
    "    'iteration': 'count',\n",
    "    'total_syscall_events': 'mean',\n",
    "    'iouring_hits': 'mean'\n",
    "}).round(1)\n",
    "summary.columns = ['Runs', 'Avg Syscall Events', 'Avg io_uring Events']\n",
    "\n",
    "print('='*60)\n",
    "print('TEST CASE SUMMARY')\n",
    "print('='*60)\n",
    "print(summary.to_string())\n",
    "print('\\nNote: High syscall events in all cases reflect background system activity.')\n",
    "print('The key metric is whether SPECIFIC test file operations appear in audit logs.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "key-finding-header",
   "metadata": {},
   "source": [
    "## 3. Key Finding: io_uring Setup Detection\n",
    "\n",
    "**Critical Insight:** While io_uring *setup* syscalls (`io_uring_setup`, `io_uring_enter`) are detectable, the actual I/O operations performed through the submission queue are invisible to syscall monitoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup-detection",
   "metadata": {},
   "outputs": [],
   "source": [
    "# io_uring setup detection rate by method\n",
    "setup_rates = df.groupby(['case', 'method'])['iouring_setup_detected'].mean() * 100\n",
    "setup_df = setup_rates.reset_index()\n",
    "setup_df.columns = ['Test Case', 'Method', 'io_uring Setup Detection %']\n",
    "\n",
    "print('='*60)\n",
    "print('io_uring SETUP SYSCALL DETECTION')\n",
    "print('='*60)\n",
    "for _, row in setup_df.iterrows():\n",
    "    marker = '✓' if row['io_uring Setup Detection %'] > 0 else '✗'\n",
    "    print(f\"{marker} {row['Test Case']}: {row['io_uring Setup Detection %']:.0f}%\")\n",
    "\n",
    "print('\\n>>> FINDING: io_uring initialization is DETECTABLE (behavioral indicator)')\n",
    "print('>>> However, this only tells defenders THAT io_uring was used, not WHAT it did.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup-viz",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 1: io_uring Setup Detection\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "cases = df.groupby('case')['iouring_setup_detected'].mean() * 100\n",
    "cases = cases.sort_values(ascending=True)\n",
    "colors = [COLORS['io_uring'] if 'uring' in c else COLORS['traditional'] for c in cases.index]\n",
    "\n",
    "bars = ax.barh(range(len(cases)), cases.values, color=colors, edgecolor='black', alpha=0.8)\n",
    "ax.set_yticks(range(len(cases)))\n",
    "ax.set_yticklabels(cases.index)\n",
    "ax.set_xlabel('Detection Rate (%)')\n",
    "ax.set_title('io_uring Setup Syscall Detection Rate', fontweight='bold', fontsize=14)\n",
    "ax.set_xlim(0, 110)\n",
    "\n",
    "for bar, val in zip(bars, cases.values):\n",
    "    ax.text(val + 2, bar.get_y() + bar.get_height()/2, f'{val:.0f}%', va='center', fontweight='bold')\n",
    "\n",
    "from matplotlib.patches import Patch\n",
    "ax.legend(handles=[Patch(color=COLORS['traditional'], label='Traditional (expected: 0%)'),\n",
    "                   Patch(color=COLORS['io_uring'], label='io_uring (expected: 100%)')], loc='lower right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'fig1_iouring_setup_detection.png', dpi=300, bbox_inches='tight')\n",
    "print(f'Saved: {FIGURES_DIR}/fig1_iouring_setup_detection.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "evasion-header",
   "metadata": {},
   "source": [
    "## 4. Primary Finding: Operation-Level Evasion\n",
    "\n",
    "The `path_hits` metric tracks whether the **specific test file** appeared in audit logs. This is the definitive evasion metric:\n",
    "- **Traditional syscalls**: File path SHOULD appear in audit logs\n",
    "- **io_uring operations**: File path should NOT appear (evasion confirmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "path-evasion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to file operations where path tracking is applicable\n",
    "file_ops = df[df['path_hits'] >= 0].copy()\n",
    "\n",
    "if len(file_ops) > 0:\n",
    "    path_detection = file_ops.groupby('case').agg({\n",
    "        'path_visible': ['sum', 'count', 'mean']\n",
    "    })\n",
    "    path_detection.columns = ['Detected', 'Total', 'Detection Rate']\n",
    "    path_detection['Detection Rate'] = (path_detection['Detection Rate'] * 100).round(1)\n",
    "    path_detection['Evasion Rate'] = 100 - path_detection['Detection Rate']\n",
    "    path_detection['Method'] = path_detection.index.map(lambda x: 'io_uring' if 'uring' in x else 'traditional')\n",
    "    \n",
    "    print('='*60)\n",
    "    print('FILE OPERATION VISIBILITY IN AUDIT LOGS')\n",
    "    print('='*60)\n",
    "    print(path_detection[['Method', 'Detected', 'Total', 'Detection Rate', 'Evasion Rate']].to_string())\n",
    "    \n",
    "    # Calculate evasion delta\n",
    "    trad_rate = file_ops[file_ops['method'] == 'traditional']['path_visible'].mean() * 100\n",
    "    uring_rate = file_ops[file_ops['method'] == 'io_uring']['path_visible'].mean() * 100\n",
    "    \n",
    "    print(f'\\n>>> Traditional file operations visible: {trad_rate:.1f}%')\n",
    "    print(f'>>> io_uring file operations visible: {uring_rate:.1f}%')\n",
    "    print(f'>>> EVASION DELTA: {trad_rate - uring_rate:.1f}% detection reduction')\n",
    "    \n",
    "    path_detection.to_csv(TABLES_DIR / 'path_detection.csv')\n",
    "else:\n",
    "    print('No path-specific data available in this dataset.')\n",
    "    trad_rate, uring_rate = 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "evasion-viz",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 2: Path Detection Comparison (KEY FIGURE)\n",
    "file_ops = df[df['path_hits'] >= 0]\n",
    "\n",
    "if len(file_ops) > 0:\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    path_rates = file_ops.groupby('case')['path_visible'].mean() * 100\n",
    "    path_rates = path_rates.reindex(sorted(path_rates.index, key=lambda x: (0 if 'traditional' in x else 1, x)))\n",
    "    \n",
    "    colors = [COLORS['traditional'] if 'traditional' in c else COLORS['io_uring'] for c in path_rates.index]\n",
    "    bars = ax.bar(range(len(path_rates)), path_rates.values, color=colors, edgecolor='black', alpha=0.8)\n",
    "    \n",
    "    ax.set_xticks(range(len(path_rates)))\n",
    "    ax.set_xticklabels(path_rates.index, rotation=45, ha='right')\n",
    "    ax.set_ylabel('File Path Visibility in Audit Logs (%)')\n",
    "    ax.set_title('File Operation Detection: Traditional vs io_uring', fontweight='bold', fontsize=14)\n",
    "    ax.set_ylim(0, 110)\n",
    "    \n",
    "    for bar, val in zip(bars, path_rates.values):\n",
    "        label = f'{val:.0f}%' if val > 0 else 'EVADED'\n",
    "        color = 'black' if val > 0 else 'red'\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 3, label, \n",
    "                ha='center', fontweight='bold', color=color)\n",
    "    \n",
    "    ax.axhline(y=50, color='orange', linestyle='--', alpha=0.7, label='50% threshold')\n",
    "    ax.legend(handles=[Patch(color=COLORS['traditional'], label='Traditional'),\n",
    "                       Patch(color=COLORS['io_uring'], label='io_uring')], loc='upper right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(FIGURES_DIR / 'fig2_path_detection.png', dpi=300, bbox_inches='tight')\n",
    "    print(f'Saved: {FIGURES_DIR}/fig2_path_detection.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forensic-header",
   "metadata": {},
   "source": [
    "## 5. Forensic Visibility Analysis\n",
    "\n",
    "Beyond detection rates, the **quality of forensic information** differs dramatically between traditional syscalls and io_uring operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forensic",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('='*70)\n",
    "print('FORENSIC VISIBILITY COMPARISON')\n",
    "print('='*70)\n",
    "\n",
    "forensic_data = {\n",
    "    'Information Type': ['File Path Accessed', 'Network Target (IP:Port)', 'Data Size', \n",
    "                         'Operation Timestamp', 'Process ID', 'io_uring Usage'],\n",
    "    'Traditional Syscalls': ['✓ YES', '✓ YES', '✓ YES', '✓ YES', '✓ YES', 'N/A'],\n",
    "    'io_uring Operations': ['✗ NO', '✗ NO', '✗ NO', '✗ NO (operation)', '✓ YES (setup only)', '✓ YES']\n",
    "}\n",
    "\n",
    "forensic_df = pd.DataFrame(forensic_data)\n",
    "print(forensic_df.to_string(index=False))\n",
    "\n",
    "print('\\n>>> IMPACT: Incident responders investigating io_uring-based attacks cannot determine:')\n",
    "print('    - What files were accessed or exfiltrated')\n",
    "print('    - What network connections were established')\n",
    "print('    - What data was transferred')\n",
    "print('\\n>>> They can ONLY see that io_uring was initialized by the process.')\n",
    "\n",
    "forensic_df.to_csv(TABLES_DIR / 'forensic_visibility.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparison-header",
   "metadata": {},
   "source": [
    "## 6. Syscall Event Volume Comparison\n",
    "\n",
    "Comparing the volume of audit events generated by each test case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "volume-comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare syscall volumes by operation type\n",
    "volume = df.groupby(['operation', 'method']).agg({\n",
    "    'file_hits': 'mean',\n",
    "    'net_hits': 'mean',\n",
    "    'exec_hits': 'mean',\n",
    "    'iouring_hits': 'mean',\n",
    "    'total_syscall_events': 'mean'\n",
    "}).round(1)\n",
    "\n",
    "print('='*60)\n",
    "print('AVERAGE AUDIT EVENTS PER TEST RUN')\n",
    "print('='*60)\n",
    "print(volume.to_string())\n",
    "print('\\nNote: High event counts in both methods reflect background system activity.')\n",
    "print('The io_uring_hits column shows setup syscall detection (present only in io_uring tests).')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "volume-viz",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 3: Side-by-side comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: Total syscall events\n",
    "ax1 = axes[0]\n",
    "ops = ['File Write', 'File Read', 'Network']\n",
    "trad_cases = ['file_io_traditional', 'read_file_traditional', 'net_connect_traditional']\n",
    "uring_cases = ['file_io_uring', 'openat_uring', 'net_connect_uring']\n",
    "\n",
    "t_vals = [df[df['case']==c]['total_syscall_events'].mean() for c in trad_cases]\n",
    "u_vals = [df[df['case']==c]['total_syscall_events'].mean() for c in uring_cases]\n",
    "\n",
    "x = np.arange(len(ops))\n",
    "ax1.bar(x - 0.2, t_vals, 0.4, label='Traditional', color=COLORS['traditional'], edgecolor='black')\n",
    "ax1.bar(x + 0.2, u_vals, 0.4, label='io_uring', color=COLORS['io_uring'], edgecolor='black')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(ops)\n",
    "ax1.set_ylabel('Mean Audit Events')\n",
    "ax1.set_title('Total Syscall Events (includes background noise)', fontweight='bold')\n",
    "ax1.legend()\n",
    "\n",
    "# Right: io_uring-specific events\n",
    "ax2 = axes[1]\n",
    "t_uring = [df[df['case']==c]['iouring_hits'].mean() for c in trad_cases]\n",
    "u_uring = [df[df['case']==c]['iouring_hits'].mean() for c in uring_cases]\n",
    "\n",
    "ax2.bar(x - 0.2, t_uring, 0.4, label='Traditional', color=COLORS['traditional'], edgecolor='black')\n",
    "ax2.bar(x + 0.2, u_uring, 0.4, label='io_uring', color=COLORS['io_uring'], edgecolor='black')\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels(ops)\n",
    "ax2.set_ylabel('Mean io_uring Events')\n",
    "ax2.set_title('io_uring Setup Detection (behavioral indicator)', fontweight='bold')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'fig3_syscall_comparison.png', dpi=300, bbox_inches='tight')\n",
    "print(f'Saved: {FIGURES_DIR}/fig3_syscall_comparison.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "heatmap-header",
   "metadata": {},
   "source": [
    "## 7. Detection Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "heatmap",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 4: Detection Heatmap\n",
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "\n",
    "hm_data = df.groupby('case').apply(lambda x: pd.Series({\n",
    "    'File Syscalls': (x['file_hits'] > 0).mean() * 100,\n",
    "    'Network Syscalls': (x['net_hits'] > 0).mean() * 100,\n",
    "    'Exec Syscalls': (x['exec_hits'] > 0).mean() * 100,\n",
    "    'io_uring Setup': (x['iouring_hits'] > 0).mean() * 100,\n",
    "    'Path Visible': (x['path_hits'] > 0).mean() * 100 if (x['path_hits'] >= 0).any() else np.nan\n",
    "}))\n",
    "\n",
    "# Sort: traditional first, then io_uring\n",
    "hm_data = hm_data.reindex(sorted(hm_data.index, key=lambda x: (0 if 'traditional' in x else 1, x)))\n",
    "\n",
    "# Custom colormap: red for low (evasion), green for high (detected)\n",
    "sns.heatmap(hm_data, annot=True, fmt='.0f', cmap='RdYlGn', vmin=0, vmax=100,\n",
    "            cbar_kws={'label': 'Detection Rate (%)'}, ax=ax, linewidths=0.5)\n",
    "ax.set_title('Detection Rate Heatmap by Test Case and Event Type', fontweight='bold', fontsize=14)\n",
    "ax.set_ylabel('Test Case')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'fig4_detection_heatmap.png', dpi=300, bbox_inches='tight')\n",
    "print(f'Saved: {FIGURES_DIR}/fig4_detection_heatmap.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defense-header",
   "metadata": {},
   "source": [
    "## 8. Defensive Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defense",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('='*70)\n",
    "print('DEFENSIVE RECOMMENDATIONS')\n",
    "print('='*70)\n",
    "\n",
    "recommendations = [\n",
    "    ('HIGH', 'Monitor io_uring setup syscalls', \n",
    "     'auditctl -a always,exit -S io_uring_setup,io_uring_enter -k iouring_detect',\n",
    "     'Detects io_uring initialization as behavioral indicator'),\n",
    "    \n",
    "    ('HIGH', 'Deploy eBPF-based monitoring',\n",
    "     'Tools: Falco, Tracee, bpftrace with io_uring probes',\n",
    "     'eBPF can trace io_uring operations at kernel level'),\n",
    "    \n",
    "    ('MEDIUM', 'Baseline io_uring usage per application',\n",
    "     'Monitor which processes typically use io_uring',\n",
    "     'Unexpected io_uring from non-DB/non-server apps is suspicious'),\n",
    "    \n",
    "    ('MEDIUM', 'Consider kernel.io_uring_disabled sysctl',\n",
    "     'sysctl kernel.io_uring_disabled=2',\n",
    "     'Disables io_uring system-wide (may break some applications)'),\n",
    "    \n",
    "    ('LOW', 'Network-level detection',\n",
    "     'Monitor network traffic at firewall/IDS level',\n",
    "     'Even if endpoint misses it, network sees the traffic')\n",
    "]\n",
    "\n",
    "rec_df = pd.DataFrame(recommendations, columns=['Priority', 'Recommendation', 'Implementation', 'Rationale'])\n",
    "print(rec_df.to_string(index=False))\n",
    "rec_df.to_csv(TABLES_DIR / 'defensive_recommendations.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-header",
   "metadata": {},
   "source": [
    "## 9. Executive Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('='*70)\n",
    "print('EXECUTIVE SUMMARY')\n",
    "print('='*70)\n",
    "\n",
    "# Calculate key metrics\n",
    "trad_df = df[df['method'] == 'traditional']\n",
    "uring_df = df[df['method'] == 'io_uring']\n",
    "\n",
    "uring_setup_rate = uring_df['iouring_setup_detected'].mean() * 100\n",
    "trad_setup_rate = trad_df['iouring_setup_detected'].mean() * 100\n",
    "\n",
    "print(f'\\nExperiment: {len(df)} test runs across {df.iteration.nunique()} iterations')\n",
    "print(f'Test cases: {df[\"case\"].nunique()} ({len(trad_df)} traditional, {len(uring_df)} io_uring)')\n",
    "\n",
    "print(f'\\n--- KEY METRICS ---')\n",
    "print(f'io_uring setup detection (io_uring cases): {uring_setup_rate:.1f}%')\n",
    "print(f'io_uring setup detection (traditional cases): {trad_setup_rate:.1f}%')\n",
    "\n",
    "# Path-based evasion if available\n",
    "file_ops = df[df['path_hits'] >= 0]\n",
    "if len(file_ops) > 0:\n",
    "    trad_path = (file_ops[file_ops['method'] == 'traditional']['path_visible']).mean() * 100\n",
    "    uring_path = (file_ops[file_ops['method'] == 'io_uring']['path_visible']).mean() * 100\n",
    "    print(f'\\nFile operation path visibility (traditional): {trad_path:.1f}%')\n",
    "    print(f'File operation path visibility (io_uring): {uring_path:.1f}%')\n",
    "    print(f'\\n>>> EVASION EFFECTIVENESS: {trad_path - uring_path:.1f}%')\n",
    "\n",
    "print('\\n--- CONCLUSIONS ---')\n",
    "print('1. io_uring SETUP is detectable via io_uring_setup/io_uring_enter syscalls')\n",
    "print('2. io_uring OPERATIONS (read/write/connect) bypass syscall-based monitoring')\n",
    "print('3. Defenders can see THAT io_uring was used, but NOT what operations were performed')\n",
    "print('4. This creates a forensic blind spot for incident response')\n",
    "\n",
    "print('\\n--- IMPLICATIONS ---')\n",
    "print('• EDR solutions relying solely on syscall tracing have a detection gap')\n",
    "print('• Attackers can use io_uring for stealthy file access and network communication')\n",
    "print('• eBPF-based monitoring is recommended to close this visibility gap')\n",
    "\n",
    "print('='*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save-summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save summary statistics\n",
    "summary_stats = {\n",
    "    'Metric': [\n",
    "        'Total Test Runs',\n",
    "        'Iterations',\n",
    "        'Traditional Test Runs',\n",
    "        'io_uring Test Runs',\n",
    "        'io_uring Setup Detection (io_uring cases)',\n",
    "        'io_uring Setup Detection (traditional cases)',\n",
    "    ],\n",
    "    'Value': [\n",
    "        len(df),\n",
    "        df.iteration.nunique(),\n",
    "        len(trad_df),\n",
    "        len(uring_df),\n",
    "        f'{uring_setup_rate:.1f}%',\n",
    "        f'{trad_setup_rate:.1f}%',\n",
    "    ]\n",
    "}\n",
    "\n",
    "if len(file_ops) > 0:\n",
    "    summary_stats['Metric'].extend([\n",
    "        'Path Visibility (traditional)',\n",
    "        'Path Visibility (io_uring)',\n",
    "        'Evasion Effectiveness'\n",
    "    ])\n",
    "    summary_stats['Value'].extend([\n",
    "        f'{trad_path:.1f}%',\n",
    "        f'{uring_path:.1f}%',\n",
    "        f'{trad_path - uring_path:.1f}%'\n",
    "    ])\n",
    "\n",
    "summary_df = pd.DataFrame(summary_stats)\n",
    "summary_df.to_csv(TABLES_DIR / 'summary_statistics.csv', index=False)\n",
    "\n",
    "print(f'\\n' + '='*70)\n",
    "print('OUTPUT FILES')\n",
    "print('='*70)\n",
    "print(f'\\nFigures saved to: {FIGURES_DIR}/')\n",
    "for f in sorted(FIGURES_DIR.glob('*.png')):\n",
    "    print(f'  - {f.name}')\n",
    "\n",
    "print(f'\\nTables saved to: {TABLES_DIR}/')\n",
    "for f in sorted(TABLES_DIR.glob('*.csv')):\n",
    "    print(f'  - {f.name}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
