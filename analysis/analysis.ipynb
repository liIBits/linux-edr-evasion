{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CSC 786 – EDR Evasion Analysis: io_uring vs Traditional Syscalls\n",
        "\n",
        "**Author:** Michael Mendoza  \n",
        "**Course:** CSC 786 – Computer Science Problems  \n",
        "**Institution:** Dakota State University\n",
        "\n",
        "---\n",
        "\n",
        "## Overview\n",
        "\n",
        "This notebook analyzes experimental results comparing EDR visibility between:\n",
        "- **Traditional syscall binaries** (baseline)\n",
        "- **io_uring-based binaries** (evasion variant)\n",
        "\n",
        "## Metrics Evaluated\n",
        "\n",
        "### Primary Metrics\n",
        "1. **Detection Rate** – Proportion of runs that produced at least one audit hit or Wazuh alert\n",
        "2. **False Negative Rate** – Proportion of runs where behavior occurred but no alert was generated\n",
        "3. **Mean Audit Hits** – Average syscall-related audit events per run\n",
        "\n",
        "### Secondary Metrics\n",
        "4. **Signal Fidelity** – Presence of contextual information in detections\n",
        "5. **Evasion Delta** – Difference in detection rates between traditional and io_uring\n",
        "\n",
        "## MITRE ATT&CK Mapping\n",
        "- **T1059** – Command and Scripting Interpreter (exec_cmd)\n",
        "- **T1071** – Application Layer Protocol (net_connect)\n",
        "- **T1005** – Data from Local System (file_io, read_file)\n",
        "- **T1562.001** – Impair Defenses: Disable or Modify Tools (io_uring evasion)\n",
        "\n",
        "---"
      ],
      "id": "header"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0) Setup and Configuration"
      ],
      "id": "setup-header"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies if needed (uncomment if running fresh)\n",
        "# !pip install pandas matplotlib seaborn scipy tabulate\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set style for publication-quality figures\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "sns.set_palette('colorblind')\n",
        "plt.rcParams['figure.figsize'] = (10, 6)\n",
        "plt.rcParams['font.size'] = 11\n",
        "plt.rcParams['axes.titlesize'] = 14\n",
        "plt.rcParams['axes.labelsize'] = 12\n",
        "\n",
        "# Create output directories\n",
        "RESULTS_DIR = Path('results')\n",
        "FIGURES_DIR = RESULTS_DIR / 'figures'\n",
        "TABLES_DIR = RESULTS_DIR / 'tables'\n",
        "\n",
        "for d in [RESULTS_DIR, FIGURES_DIR, TABLES_DIR]:\n",
        "    d.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print('Output directories created:')\n",
        "print(f'  Figures: {FIGURES_DIR}')\n",
        "print(f'  Tables:  {TABLES_DIR}')"
      ],
      "id": "setup"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Load Experimental Data"
      ],
      "id": "load-header"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Find the most recent CSV file\n",
        "processed_dir = Path('../data/processed')\n",
        "\n",
        "# Handle running from different directories\n",
        "if not processed_dir.exists():\n",
        "    processed_dir = Path('data/processed')\n",
        "if not processed_dir.exists():\n",
        "    processed_dir = Path('../data/processed')\n",
        "\n",
        "csvs = sorted(processed_dir.glob('runs_*.csv'))\n",
        "\n",
        "if not csvs:\n",
        "    raise FileNotFoundError(\n",
        "        f'No runs_*.csv found in {processed_dir}\\n'\n",
        "        'Run the experiment first: sudo ./run_experiment.sh 30'\n",
        "    )\n",
        "\n",
        "# Use the most recent CSV\n",
        "csv_path = csvs[-1]\n",
        "print(f'Loading: {csv_path}')\n",
        "print(f'Available CSV files: {len(csvs)}')\n",
        "\n",
        "# Load data\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "print(f'\\nDataset shape: {df.shape[0]} rows × {df.shape[1]} columns')\n",
        "print(f'Columns: {list(df.columns)}')\n",
        "df.head(10)"
      ],
      "id": "load-data"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Data Validation and Preprocessing"
      ],
      "id": "validation-header"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Validate required columns\n",
        "required_cols = ['iteration', 'case', 'file_hits', 'net_hits', 'exec_hits', 'wazuh_alerts']\n",
        "missing = [c for c in required_cols if c not in df.columns]\n",
        "\n",
        "if missing:\n",
        "    print(f'WARNING: Missing columns: {missing}')\n",
        "    print('Some analyses may be limited.')\n",
        "else:\n",
        "    print('✓ All required columns present')\n",
        "\n",
        "# Basic stats\n",
        "print(f'\\nIterations: {df[\"iteration\"].nunique()}')\n",
        "print(f'Test cases: {df[\"case\"].nunique()}')\n",
        "print(f'\\nCases in dataset:')\n",
        "for case in sorted(df['case'].unique()):\n",
        "    count = len(df[df['case'] == case])\n",
        "    print(f'  - {case}: {count} runs')"
      ],
      "id": "validation"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add derived columns for analysis\n",
        "\n",
        "# Classify each case as traditional or io_uring\n",
        "def classify_case(case_name):\n",
        "    if 'uring' in case_name.lower():\n",
        "        return 'io_uring'\n",
        "    else:\n",
        "        return 'traditional'\n",
        "\n",
        "df['method'] = df['case'].apply(classify_case)\n",
        "\n",
        "# Classify by operation type\n",
        "def classify_operation(case_name):\n",
        "    case_lower = case_name.lower()\n",
        "    if 'file_io' in case_lower:\n",
        "        return 'File I/O (write)'\n",
        "    elif 'read_file' in case_lower or 'openat' in case_lower:\n",
        "        return 'File I/O (read)'\n",
        "    elif 'net' in case_lower:\n",
        "        return 'Network'\n",
        "    elif 'exec' in case_lower:\n",
        "        return 'Process Exec'\n",
        "    else:\n",
        "        return 'Other'\n",
        "\n",
        "df['operation'] = df['case'].apply(classify_operation)\n",
        "\n",
        "# Calculate total audit hits (sum of all audit sources)\n",
        "df['total_audit_hits'] = df['file_hits'] + df['net_hits'] + df['exec_hits']\n",
        "\n",
        "# Detection flags (binary: was it detected?)\n",
        "df['audit_detected'] = df['total_audit_hits'] > 0\n",
        "df['wazuh_detected'] = df['wazuh_alerts'] > 0\n",
        "df['any_detected'] = df['audit_detected'] | df['wazuh_detected']\n",
        "\n",
        "print('Derived columns added:')\n",
        "print('  - method: traditional vs io_uring')\n",
        "print('  - operation: File I/O, Network, Process Exec')\n",
        "print('  - total_audit_hits: sum of all audit hits')\n",
        "print('  - audit_detected, wazuh_detected, any_detected: boolean flags')\n",
        "\n",
        "df[['case', 'method', 'operation', 'total_audit_hits', 'audit_detected']].head(10)"
      ],
      "id": "preprocessing"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Primary Metric: Detection Rate\n",
        "\n",
        "**Definition:** Proportion of runs that produced at least one audit hit or Wazuh alert.\n",
        "\n",
        "This directly answers: *Did the EDR see the activity?*"
      ],
      "id": "detection-rate-header"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate detection rates by case\n",
        "detection_rates = df.groupby('case').agg({\n",
        "    'audit_detected': 'mean',\n",
        "    'wazuh_detected': 'mean',\n",
        "    'any_detected': 'mean',\n",
        "    'iteration': 'count'\n",
        "}).rename(columns={\n",
        "    'audit_detected': 'Audit Detection Rate',\n",
        "    'wazuh_detected': 'Wazuh Detection Rate', \n",
        "    'any_detected': 'Overall Detection Rate',\n",
        "    'iteration': 'N (runs)'\n",
        "})\n",
        "\n",
        "# Add method classification\n",
        "detection_rates['Method'] = detection_rates.index.map(classify_case)\n",
        "\n",
        "# Format as percentages for display\n",
        "detection_rates_display = detection_rates.copy()\n",
        "for col in ['Audit Detection Rate', 'Wazuh Detection Rate', 'Overall Detection Rate']:\n",
        "    detection_rates_display[col] = (detection_rates_display[col] * 100).round(1).astype(str) + '%'\n",
        "\n",
        "print('=== Detection Rates by Test Case ===')\n",
        "print(detection_rates_display.to_string())\n",
        "\n",
        "# Save to CSV\n",
        "detection_rates.to_csv(TABLES_DIR / 'detection_rates_by_case.csv')\n",
        "print(f'\\nSaved: {TABLES_DIR}/detection_rates_by_case.csv')"
      ],
      "id": "detection-rate"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Detection rates by method (traditional vs io_uring)\n",
        "method_rates = df.groupby('method').agg({\n",
        "    'audit_detected': ['mean', 'sum', 'count'],\n",
        "    'wazuh_detected': ['mean', 'sum'],\n",
        "}).round(4)\n",
        "\n",
        "method_rates.columns = ['Audit Rate', 'Audit Detections', 'Total Runs', 'Wazuh Rate', 'Wazuh Detections']\n",
        "\n",
        "print('=== Detection Rates: Traditional vs io_uring ===')\n",
        "print(method_rates.to_string())\n",
        "\n",
        "# Calculate evasion effectiveness\n",
        "if 'traditional' in method_rates.index and 'io_uring' in method_rates.index:\n",
        "    trad_rate = method_rates.loc['traditional', 'Audit Rate']\n",
        "    uring_rate = method_rates.loc['io_uring', 'Audit Rate']\n",
        "    evasion_delta = trad_rate - uring_rate\n",
        "    evasion_pct = (evasion_delta / trad_rate * 100) if trad_rate > 0 else 0\n",
        "    \n",
        "    print(f'\\n=== Evasion Effectiveness ===')\n",
        "    print(f'Traditional detection rate: {trad_rate:.1%}')\n",
        "    print(f'io_uring detection rate:    {uring_rate:.1%}')\n",
        "    print(f'Detection reduction:        {evasion_delta:.1%} ({evasion_pct:.1f}% evasion)')"
      ],
      "id": "method-comparison"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) Primary Metric: False Negative Rate\n",
        "\n",
        "**Definition:** Proportion of runs where behavior occurred but no alert was generated.\n",
        "\n",
        "For io_uring cases, we *expect* high false negatives (the evasion is working).  \n",
        "For traditional cases, false negatives indicate gaps in audit rules."
      ],
      "id": "false-neg-header"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# False negative = behavior occurred but not detected\n",
        "# Since all runs execute the behavior, FN rate = 1 - detection rate\n",
        "\n",
        "fn_rates = df.groupby('case').agg({\n",
        "    'audit_detected': lambda x: 1 - x.mean(),\n",
        "    'wazuh_detected': lambda x: 1 - x.mean(),\n",
        "}).rename(columns={\n",
        "    'audit_detected': 'Audit FN Rate',\n",
        "    'wazuh_detected': 'Wazuh FN Rate'\n",
        "})\n",
        "\n",
        "fn_rates['Method'] = fn_rates.index.map(classify_case)\n",
        "fn_rates['Operation'] = fn_rates.index.map(classify_operation)\n",
        "\n",
        "print('=== False Negative Rates by Test Case ===')\n",
        "print('(Higher = more evasion / less visibility)\\n')\n",
        "\n",
        "fn_display = fn_rates.copy()\n",
        "fn_display['Audit FN Rate'] = (fn_display['Audit FN Rate'] * 100).round(1).astype(str) + '%'\n",
        "fn_display['Wazuh FN Rate'] = (fn_display['Wazuh FN Rate'] * 100).round(1).astype(str) + '%'\n",
        "print(fn_display.to_string())\n",
        "\n",
        "fn_rates.to_csv(TABLES_DIR / 'false_negative_rates.csv')\n",
        "print(f'\\nSaved: {TABLES_DIR}/false_negative_rates.csv')"
      ],
      "id": "false-negatives"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) Validation: Confirming io_uring Bypasses Syscall Traces\n",
        "### Validation Approach\n",
        "\n",
        "We validate bypass by comparing **audit hit counts** between paired operations:\n",
        "\n",
        "| Operation | Traditional Binary | io_uring Binary | Expected Result |\n",
        "|-----------|-------------------|-----------------|------------------|\n",
        "| File Write | file_io_traditional | file_io_uring | Trad >> Uring |\n",
        "| File Read | read_file_traditional | openat_uring | Trad >> Uring |\n",
        "| Network | net_connect_traditional | net_connect_uring | Trad >> Uring |\n",
        "\n",
        "If io_uring successfully bypasses syscall tracing:\n",
        "1. Traditional binaries should generate **significantly more** audit hits\n",
        "2. io_uring binaries should generate **near-zero** audit hits for file/net operations\n",
        "3. The difference should be **statistically significant**"
      ],
      "id": "validation-header2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define paired comparisons\n",
        "pairs = [\n",
        "    ('file_io_traditional', 'file_io_uring', 'file_hits', 'File Write'),\n",
        "    ('read_file_traditional', 'openat_uring', 'file_hits', 'File Read'),\n",
        "    ('net_connect_traditional', 'net_connect_uring', 'net_hits', 'Network Connect'),\n",
        "]\n",
        "\n",
        "validation_results = []\n",
        "\n",
        "print('=== Syscall Bypass Validation ===' )\n",
        "print('Comparing audit hits between traditional and io_uring binaries\\n')\n",
        "\n",
        "for trad_case, uring_case, metric_col, op_name in pairs:\n",
        "    trad_data = df[df['case'] == trad_case][metric_col]\n",
        "    uring_data = df[df['case'] == uring_case][metric_col]\n",
        "    \n",
        "    if len(trad_data) == 0 or len(uring_data) == 0:\n",
        "        print(f'{op_name}: SKIPPED (missing data)')\n",
        "        continue\n",
        "    \n",
        "    trad_mean = trad_data.mean()\n",
        "    uring_mean = uring_data.mean()\n",
        "    \n",
        "    # Statistical test (Mann-Whitney U for non-normal distributions)\n",
        "    stat, p_value = stats.mannwhitneyu(trad_data, uring_data, alternative='greater')\n",
        "    \n",
        "    # Effect size (difference in means)\n",
        "    effect = trad_mean - uring_mean\n",
        "    \n",
        "    # Bypass confirmed if uring has significantly fewer hits\n",
        "    bypass_confirmed = p_value < 0.05 and effect > 0\n",
        "    \n",
        "    result = {\n",
        "        'Operation': op_name,\n",
        "        'Traditional Mean': round(trad_mean, 2),\n",
        "        'io_uring Mean': round(uring_mean, 2),\n",
        "        'Difference': round(effect, 2),\n",
        "        'p-value': f'{p_value:.4f}',\n",
        "        'Bypass Confirmed': '✓ YES' if bypass_confirmed else '✗ NO'\n",
        "    }\n",
        "    validation_results.append(result)\n",
        "    \n",
        "    print(f'--- {op_name} ---')\n",
        "    print(f'  Traditional ({trad_case}): mean = {trad_mean:.2f} hits')\n",
        "    print(f'  io_uring ({uring_case}):    mean = {uring_mean:.2f} hits')\n",
        "    print(f'  Difference: {effect:.2f} (p = {p_value:.4f})')\n",
        "    print(f'  Syscall bypass confirmed: {\"YES ✓\" if bypass_confirmed else \"NO ✗\"}')\n",
        "    print()\n",
        "\n",
        "# Create validation summary table\n",
        "validation_df = pd.DataFrame(validation_results)\n",
        "validation_df.to_csv(TABLES_DIR / 'syscall_bypass_validation.csv', index=False)\n",
        "print(f'Saved: {TABLES_DIR}/syscall_bypass_validation.csv')"
      ],
      "id": "bypass-validation"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6) Descriptive Statistics: Mean Audit Hits"
      ],
      "id": "descriptive-header"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Comprehensive summary statistics\n",
        "metric_cols = ['file_hits', 'net_hits', 'exec_hits', 'total_audit_hits', 'wazuh_alerts']\n",
        "\n",
        "summary_stats = df.groupby('case')[metric_cols].agg(['mean', 'std', 'min', 'max']).round(2)\n",
        "\n",
        "print('=== Descriptive Statistics by Test Case ===')\n",
        "print(summary_stats.to_string())\n",
        "\n",
        "summary_stats.to_csv(TABLES_DIR / 'descriptive_statistics.csv')\n",
        "print(f'\\nSaved: {TABLES_DIR}/descriptive_statistics.csv')"
      ],
      "id": "descriptive-stats"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simplified mean comparison table (good for papers)\n",
        "means_table = df.groupby('case')[metric_cols].mean().round(2)\n",
        "means_table['Method'] = means_table.index.map(classify_case)\n",
        "means_table['Operation'] = means_table.index.map(classify_operation)\n",
        "\n",
        "# Reorder columns\n",
        "means_table = means_table[['Method', 'Operation', 'file_hits', 'net_hits', 'exec_hits', 'total_audit_hits', 'wazuh_alerts']]\n",
        "\n",
        "print('=== Mean Values by Test Case (for publication) ===')\n",
        "print(means_table.to_string())\n",
        "\n",
        "means_table.to_csv(TABLES_DIR / 'mean_hits_by_case.csv')\n",
        "print(f'\\nSaved: {TABLES_DIR}/mean_hits_by_case.csv')"
      ],
      "id": "means-table"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7) Visualizations"
      ],
      "id": "viz-header"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Figure 1: Detection Rate Comparison (Bar Chart)\n",
        "fig, ax = plt.subplots(figsize=(12, 6))\n",
        "\n",
        "# Prepare data\n",
        "plot_data = df.groupby(['case', 'method'])['audit_detected'].mean().reset_index()\n",
        "plot_data['audit_detected'] *= 100  # Convert to percentage\n",
        "\n",
        "# Create grouped bar chart\n",
        "cases = plot_data['case'].unique()\n",
        "x = np.arange(len(cases))\n",
        "colors = ['#2ecc71' if 'traditional' in c else '#e74c3c' for c in cases]\n",
        "\n",
        "bars = ax.bar(x, plot_data.groupby('case')['audit_detected'].first(), color=colors, edgecolor='black', linewidth=0.5)\n",
        "\n",
        "ax.set_xlabel('Test Case')\n",
        "ax.set_ylabel('Audit Detection Rate (%)')\n",
        "ax.set_title('Figure 1: Audit Detection Rate by Test Case\\n(Traditional vs io_uring)', fontweight='bold')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(cases, rotation=45, ha='right')\n",
        "ax.set_ylim(0, 105)\n",
        "\n",
        "# Add value labels on bars\n",
        "for bar, val in zip(bars, plot_data.groupby('case')['audit_detected'].first()):\n",
        "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 2, f'{val:.0f}%', \n",
        "            ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "# Add legend\n",
        "from matplotlib.patches import Patch\n",
        "legend_elements = [Patch(facecolor='#2ecc71', label='Traditional (syscalls)'),\n",
        "                   Patch(facecolor='#e74c3c', label='io_uring (evasion)')]\n",
        "ax.legend(handles=legend_elements, loc='upper right')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(FIGURES_DIR / 'fig1_detection_rate_comparison.png', dpi=300, bbox_inches='tight')\n",
        "plt.savefig(FIGURES_DIR / 'fig1_detection_rate_comparison.pdf', bbox_inches='tight')\n",
        "plt.show()\n",
        "print(f'Saved: {FIGURES_DIR}/fig1_detection_rate_comparison.png')"
      ],
      "id": "fig1-detection-rate"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Figure 2: Mean Audit Hits Comparison (Grouped Bar Chart)\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "# Prepare data for paired comparison\n",
        "pair_labels = ['File Write', 'File Read', 'Network']\n",
        "trad_cases = ['file_io_traditional', 'read_file_traditional', 'net_connect_traditional']\n",
        "uring_cases = ['file_io_uring', 'openat_uring', 'net_connect_uring']\n",
        "\n",
        "trad_means = [df[df['case'] == c]['total_audit_hits'].mean() for c in trad_cases]\n",
        "uring_means = [df[df['case'] == c]['total_audit_hits'].mean() for c in uring_cases]\n",
        "\n",
        "x = np.arange(len(pair_labels))\n",
        "width = 0.35\n",
        "\n",
        "bars1 = ax.bar(x - width/2, trad_means, width, label='Traditional', color='#2ecc71', edgecolor='black')\n",
        "bars2 = ax.bar(x + width/2, uring_means, width, label='io_uring', color='#e74c3c', edgecolor='black')\n",
        "\n",
        "ax.set_xlabel('Operation Type')\n",
        "ax.set_ylabel('Mean Audit Hits')\n",
        "ax.set_title('Figure 2: Mean Audit Hits – Traditional vs io_uring\\n(Lower io_uring values indicate successful syscall bypass)', fontweight='bold')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(pair_labels)\n",
        "ax.legend()\n",
        "\n",
        "# Add value labels\n",
        "for bars in [bars1, bars2]:\n",
        "    for bar in bars:\n",
        "        height = bar.get_height()\n",
        "        ax.text(bar.get_x() + bar.get_width()/2, height + 0.5, f'{height:.1f}',\n",
        "                ha='center', va='bottom', fontsize=10)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(FIGURES_DIR / 'fig2_mean_hits_comparison.png', dpi=300, bbox_inches='tight')\n",
        "plt.savefig(FIGURES_DIR / 'fig2_mean_hits_comparison.pdf', bbox_inches='tight')\n",
        "plt.show()\n",
        "print(f'Saved: {FIGURES_DIR}/fig2_mean_hits_comparison.png')"
      ],
      "id": "fig2-mean-hits"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Figure 3: Box Plot Distribution\n",
        "fig, axes = plt.subplots(1, 3, figsize=(14, 5))\n",
        "\n",
        "comparisons = [\n",
        "    ('file_io_traditional', 'file_io_uring', 'file_hits', 'File Write Operations'),\n",
        "    ('read_file_traditional', 'openat_uring', 'file_hits', 'File Read Operations'),\n",
        "    ('net_connect_traditional', 'net_connect_uring', 'net_hits', 'Network Operations'),\n",
        "]\n",
        "\n",
        "for ax, (trad, uring, metric, title) in zip(axes, comparisons):\n",
        "    trad_data = df[df['case'] == trad][metric]\n",
        "    uring_data = df[df['case'] == uring][metric]\n",
        "    \n",
        "    bp = ax.boxplot([trad_data, uring_data], labels=['Traditional', 'io_uring'],\n",
        "                    patch_artist=True)\n",
        "    \n",
        "    bp['boxes'][0].set_facecolor('#2ecc71')\n",
        "    bp['boxes'][1].set_facecolor('#e74c3c')\n",
        "    \n",
        "    ax.set_title(title)\n",
        "    ax.set_ylabel('Audit Hits')\n",
        "\n",
        "fig.suptitle('Figure 3: Distribution of Audit Hits by Operation Type', fontweight='bold', y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.savefig(FIGURES_DIR / 'fig3_boxplot_distributions.png', dpi=300, bbox_inches='tight')\n",
        "plt.savefig(FIGURES_DIR / 'fig3_boxplot_distributions.pdf', bbox_inches='tight')\n",
        "plt.show()\n",
        "print(f'Saved: {FIGURES_DIR}/fig3_boxplot_distributions.png')"
      ],
      "id": "fig3-boxplot"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Figure 4: Heatmap of Detection Rates\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "# Prepare heatmap data\n",
        "heatmap_data = df.groupby('case')[['file_hits', 'net_hits', 'exec_hits', 'wazuh_alerts']].apply(\n",
        "    lambda x: (x > 0).mean()\n",
        ") * 100\n",
        "\n",
        "heatmap_data.columns = ['File Audit', 'Network Audit', 'Exec Audit', 'Wazuh Alerts']\n",
        "\n",
        "# Sort by method for better visualization\n",
        "heatmap_data['sort_key'] = heatmap_data.index.map(lambda x: 0 if 'traditional' in x else 1)\n",
        "heatmap_data = heatmap_data.sort_values('sort_key').drop('sort_key', axis=1)\n",
        "\n",
        "sns.heatmap(heatmap_data, annot=True, fmt='.0f', cmap='RdYlGn', \n",
        "            linewidths=0.5, ax=ax, vmin=0, vmax=100,\n",
        "            cbar_kws={'label': 'Detection Rate (%)'})\n",
        "\n",
        "ax.set_title('Figure 4: Detection Rate Heatmap by Test Case and Metric\\n(Green = High Detection, Red = Low/Evasion)', \n",
        "             fontweight='bold')\n",
        "ax.set_xlabel('Detection Source')\n",
        "ax.set_ylabel('Test Case')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(FIGURES_DIR / 'fig4_detection_heatmap.png', dpi=300, bbox_inches='tight')\n",
        "plt.savefig(FIGURES_DIR / 'fig4_detection_heatmap.pdf', bbox_inches='tight')\n",
        "plt.show()\n",
        "print(f'Saved: {FIGURES_DIR}/fig4_detection_heatmap.png')"
      ],
      "id": "fig4-heatmap"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Figure 5: Evasion Effectiveness Summary\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "\n",
        "# Calculate evasion rates by operation\n",
        "operations = ['File Write', 'File Read', 'Network']\n",
        "trad_detection = []\n",
        "uring_detection = []\n",
        "\n",
        "for trad, uring in [('file_io_traditional', 'file_io_uring'), \n",
        "                    ('read_file_traditional', 'openat_uring'),\n",
        "                    ('net_connect_traditional', 'net_connect_uring')]:\n",
        "    trad_rate = df[df['case'] == trad]['audit_detected'].mean() * 100\n",
        "    uring_rate = df[df['case'] == uring]['audit_detected'].mean() * 100\n",
        "    trad_detection.append(trad_rate)\n",
        "    uring_detection.append(uring_rate)\n",
        "\n",
        "evasion_pct = [t - u for t, u in zip(trad_detection, uring_detection)]\n",
        "\n",
        "colors = ['#3498db' if e > 0 else '#95a5a6' for e in evasion_pct]\n",
        "bars = ax.barh(operations, evasion_pct, color=colors, edgecolor='black')\n",
        "\n",
        "ax.set_xlabel('Detection Rate Reduction (%)')\n",
        "ax.set_title('Figure 5: io_uring Evasion Effectiveness\\n(Reduction in Detection Rate vs Traditional)', fontweight='bold')\n",
        "ax.axvline(x=0, color='black', linewidth=0.5)\n",
        "\n",
        "# Add value labels\n",
        "for bar, val in zip(bars, evasion_pct):\n",
        "    ax.text(val + 1, bar.get_y() + bar.get_height()/2, f'{val:.1f}%',\n",
        "            va='center', fontsize=11, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(FIGURES_DIR / 'fig5_evasion_effectiveness.png', dpi=300, bbox_inches='tight')\n",
        "plt.savefig(FIGURES_DIR / 'fig5_evasion_effectiveness.pdf', bbox_inches='tight')\n",
        "plt.show()\n",
        "print(f'Saved: {FIGURES_DIR}/fig5_evasion_effectiveness.png')"
      ],
      "id": "fig5-evasion"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8) Statistical Significance Testing"
      ],
      "id": "stats-header"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Perform statistical tests for all paired comparisons\n",
        "print('=== Statistical Significance Tests ===')\n",
        "print('H₀: Traditional and io_uring have equal audit hit distributions')\n",
        "print('H₁: Traditional has greater audit hits than io_uring')\n",
        "print('Test: Mann-Whitney U (one-tailed, α = 0.05)\\n')\n",
        "\n",
        "stat_results = []\n",
        "\n",
        "test_pairs = [\n",
        "    ('file_io_traditional', 'file_io_uring', 'file_hits'),\n",
        "    ('read_file_traditional', 'openat_uring', 'file_hits'),\n",
        "    ('net_connect_traditional', 'net_connect_uring', 'net_hits'),\n",
        "]\n",
        "\n",
        "for trad_case, uring_case, metric in test_pairs:\n",
        "    trad = df[df['case'] == trad_case][metric]\n",
        "    uring = df[df['case'] == uring_case][metric]\n",
        "    \n",
        "    if len(trad) == 0 or len(uring) == 0:\n",
        "        continue\n",
        "    \n",
        "    stat, p = stats.mannwhitneyu(trad, uring, alternative='greater')\n",
        "    \n",
        "    # Effect size (rank-biserial correlation)\n",
        "    n1, n2 = len(trad), len(uring)\n",
        "    effect_size = 1 - (2*stat)/(n1*n2)\n",
        "    \n",
        "    significant = p < 0.05\n",
        "    \n",
        "    stat_results.append({\n",
        "        'Comparison': f'{trad_case} vs {uring_case}',\n",
        "        'Metric': metric,\n",
        "        'U-statistic': stat,\n",
        "        'p-value': p,\n",
        "        'Effect Size (r)': round(effect_size, 3),\n",
        "        'Significant': 'Yes' if significant else 'No'\n",
        "    })\n",
        "    \n",
        "    print(f'{trad_case} vs {uring_case}:')\n",
        "    print(f'  U = {stat:.1f}, p = {p:.6f}, r = {effect_size:.3f}')\n",
        "    print(f'  Result: {\"SIGNIFICANT ✓\" if significant else \"Not significant\"}')\n",
        "    print()\n",
        "\n",
        "stat_df = pd.DataFrame(stat_results)\n",
        "stat_df.to_csv(TABLES_DIR / 'statistical_tests.csv', index=False)\n",
        "print(f'Saved: {TABLES_DIR}/statistical_tests.csv')"
      ],
      "id": "stat-tests"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9) MITRE ATT&CK Mapping Table"
      ],
      "id": "mitre-header"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create MITRE ATT&CK mapping table\n",
        "mitre_mapping = pd.DataFrame([\n",
        "    {\n",
        "        'Test Case': 'exec_cmd_traditional',\n",
        "        'Operation': 'Process Execution',\n",
        "        'MITRE Technique': 'T1059',\n",
        "        'Technique Name': 'Command and Scripting Interpreter',\n",
        "        'io_uring Variant': 'N/A (execve cannot use io_uring)',\n",
        "        'Evasion Possible': 'No'\n",
        "    },\n",
        "    {\n",
        "        'Test Case': 'file_io_traditional / file_io_uring',\n",
        "        'Operation': 'File Write',\n",
        "        'MITRE Technique': 'T1005',\n",
        "        'Technique Name': 'Data from Local System',\n",
        "        'io_uring Variant': 'IORING_OP_WRITE',\n",
        "        'Evasion Possible': 'Yes'\n",
        "    },\n",
        "    {\n",
        "        'Test Case': 'read_file_traditional / openat_uring',\n",
        "        'Operation': 'File Read',\n",
        "        'MITRE Technique': 'T1005',\n",
        "        'Technique Name': 'Data from Local System',\n",
        "        'io_uring Variant': 'IORING_OP_OPENAT + READ',\n",
        "        'Evasion Possible': 'Yes'\n",
        "    },\n",
        "    {\n",
        "        'Test Case': 'net_connect_traditional / net_connect_uring',\n",
        "        'Operation': 'Network Connection',\n",
        "        'MITRE Technique': 'T1071',\n",
        "        'Technique Name': 'Application Layer Protocol',\n",
        "        'io_uring Variant': 'IORING_OP_CONNECT',\n",
        "        'Evasion Possible': 'Yes'\n",
        "    },\n",
        "    {\n",
        "        'Test Case': 'All io_uring variants',\n",
        "        'Operation': 'Defense Evasion',\n",
        "        'MITRE Technique': 'T1562.001',\n",
        "        'Technique Name': 'Impair Defenses: Disable or Modify Tools',\n",
        "        'io_uring Variant': 'Syscall trace bypass',\n",
        "        'Evasion Possible': 'Yes (primary finding)'\n",
        "    },\n",
        "])\n",
        "\n",
        "print('=== MITRE ATT&CK Mapping ===')\n",
        "print(mitre_mapping.to_string(index=False))\n",
        "\n",
        "mitre_mapping.to_csv(TABLES_DIR / 'mitre_attack_mapping.csv', index=False)\n",
        "print(f'\\nSaved: {TABLES_DIR}/mitre_attack_mapping.csv')"
      ],
      "id": "mitre-mapping"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10) Executive Summary"
      ],
      "id": "summary-header"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate executive summary\n",
        "print('=' * 70)\n",
        "print('EXECUTIVE SUMMARY: EDR Evasion via io_uring')\n",
        "print('=' * 70)\n",
        "\n",
        "# Overall statistics\n",
        "total_runs = len(df)\n",
        "iterations = df['iteration'].nunique()\n",
        "cases = df['case'].nunique()\n",
        "\n",
        "print(f'\\nExperiment Overview:')\n",
        "print(f'  Total test runs: {total_runs}')\n",
        "print(f'  Iterations: {iterations}')\n",
        "print(f'  Test cases: {cases}')\n",
        "\n",
        "# Detection comparison\n",
        "trad_df = df[df['method'] == 'traditional']\n",
        "uring_df = df[df['method'] == 'io_uring']\n",
        "\n",
        "print(f'\\nDetection Rates:')\n",
        "print(f'  Traditional syscalls: {trad_df[\"audit_detected\"].mean():.1%} audit detection')\n",
        "print(f'  io_uring operations:  {uring_df[\"audit_detected\"].mean():.1%} audit detection')\n",
        "\n",
        "# Key finding\n",
        "evasion_rate = trad_df['audit_detected'].mean() - uring_df['audit_detected'].mean()\n",
        "print(f'\\nKey Finding:')\n",
        "print(f'  io_uring reduces audit detection by {evasion_rate:.1%}')\n",
        "\n",
        "print(f'\\nConclusion:')\n",
        "if evasion_rate > 0.5:\n",
        "    print('  ⚠️  SIGNIFICANT EVASION GAP CONFIRMED')\n",
        "    print('  io_uring operations successfully bypass syscall-based monitoring.')\n",
        "    print('  Defenders should implement eBPF-based monitoring or audit io_uring_enter().')\n",
        "elif evasion_rate > 0:\n",
        "    print('  ⚠️  PARTIAL EVASION GAP DETECTED')\n",
        "    print('  io_uring shows reduced visibility compared to traditional syscalls.')\n",
        "else:\n",
        "    print('  ✓ No significant evasion gap detected in this configuration.')\n",
        "\n",
        "print(f'\\nOutput Files:')\n",
        "print(f'  Figures: {FIGURES_DIR}/')\n",
        "print(f'  Tables:  {TABLES_DIR}/')\n",
        "print('=' * 70)"
      ],
      "id": "executive-summary"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# List all generated output files\n",
        "print('\\n=== Generated Output Files ===')\n",
        "\n",
        "print('\\nFigures:')\n",
        "for f in sorted(FIGURES_DIR.glob('*')):\n",
        "    print(f'  {f.name}')\n",
        "\n",
        "print('\\nTables (CSV):')\n",
        "for f in sorted(TABLES_DIR.glob('*.csv')):\n",
        "    print(f'  {f.name}')\n",
        "\n",
        "print('\\nUse these files in your final report!')"
      ],
      "id": "list-outputs"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Notes for Final Report\n",
        "\n",
        "### Key Evidence for Syscall Bypass Validation\n",
        "\n",
        "The validation section (Section 5) provides statistical evidence that io_uring truly bypasses syscall traces:\n",
        "\n",
        "1. **Quantitative difference**: Traditional binaries generate significantly more audit hits\n",
        "2. **Statistical significance**: Mann-Whitney U tests confirm the difference is not due to chance\n",
        "3. **Consistent pattern**: All three operation types (file write, file read, network) show the same pattern\n",
        "\n",
        "### Figures for Publication\n",
        "\n",
        "- **Figure 1**: Detection rate bar chart (main finding)\n",
        "- **Figure 2**: Mean audit hits comparison (quantitative evidence)\n",
        "- **Figure 3**: Box plots showing distributions (variance analysis)\n",
        "- **Figure 4**: Heatmap (comprehensive view)\n",
        "- **Figure 5**: Evasion effectiveness summary (executive summary visual)\n",
        "\n",
        "### Tables for Publication\n",
        "\n",
        "- `detection_rates_by_case.csv`: Primary detection rate metrics\n",
        "- `syscall_bypass_validation.csv`: Evidence for bypass confirmation\n",
        "- `statistical_tests.csv`: Statistical significance results\n",
        "- `mitre_attack_mapping.csv`: MITRE ATT&CK technique mapping"
      ],
      "id": "notes"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
