{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CSC 786 - EDR Evasion Analysis: io_uring vs Traditional Syscalls\n",
        "\n",
        "**Author:** Michael Mendoza | **Course:** CSC 786 | **Institution:** Dakota State University\n",
        "\n",
        "---\n",
        "\n",
        "## Metrics Evaluated\n",
        "\n",
        "### Primary Metrics (from A3)\n",
        "1. **Detection Rate** - Proportion of runs producing at least one audit hit\n",
        "2. **False Negative Rate** - Runs where behavior occurred but no alert generated\n",
        "3. **Time-to-Detection** - Time between execution and first alert\n",
        "\n",
        "### Secondary Metrics\n",
        "4. **io_uring Setup Detection** - Whether io_uring initialization is visible\n",
        "5. **Evasion Delta** - Detection rate difference (traditional - io_uring)\n",
        "\n",
        "## MITRE ATT&CK Mapping\n",
        "- T1059 - Command and Scripting Interpreter (exec_cmd)\n",
        "- T1071 - Application Layer Protocol (net_connect)\n",
        "- T1005 - Data from Local System (file_io)\n",
        "- T1562.001 - Impair Defenses (io_uring evasion)"
      ],
      "id": "header"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ["## 0) Setup"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "sns.set_palette('colorblind')\n",
        "plt.rcParams.update({'figure.figsize': (10, 6), 'font.size': 11})\n",
        "\n",
        "RESULTS_DIR = Path('results')\n",
        "FIGURES_DIR = RESULTS_DIR / 'figures'\n",
        "TABLES_DIR = RESULTS_DIR / 'tables'\n",
        "for d in [FIGURES_DIR, TABLES_DIR]: d.mkdir(parents=True, exist_ok=True)\n",
        "print(f'Output: {RESULTS_DIR}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ["## 1) Load Data"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for p in [Path('../data/processed'), Path('data/processed'), Path('../../data/processed')]:\n",
        "    if p.exists():\n",
        "        processed_dir = p\n",
        "        break\n",
        "else:\n",
        "    raise FileNotFoundError('No data/processed directory found')\n",
        "\n",
        "csvs = sorted(processed_dir.glob('runs_*.csv'))\n",
        "if not csvs: raise FileNotFoundError('No runs_*.csv found')\n",
        "\n",
        "csv_path = csvs[-1]\n",
        "print(f'Loading: {csv_path}')\n",
        "df = pd.read_csv(csv_path)\n",
        "print(f'Shape: {df.shape}')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ["## 2) Preprocessing"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add missing columns for backward compatibility\n",
        "if 'iouring_hits' not in df.columns: df['iouring_hits'] = 0\n",
        "if 'time_to_detect' not in df.columns: df['time_to_detect'] = -1\n",
        "\n",
        "# Classify cases\n",
        "df['method'] = df['case'].apply(lambda x: 'io_uring' if 'uring' in x.lower() else 'traditional')\n",
        "df['operation'] = df['case'].apply(lambda x: \n",
        "    'File Write' if 'file_io' in x.lower() else\n",
        "    'File Read' if 'read_file' in x.lower() or 'openat' in x.lower() else\n",
        "    'Network' if 'net' in x.lower() else\n",
        "    'Process Exec' if 'exec' in x.lower() else 'Other')\n",
        "\n",
        "# Derived metrics\n",
        "df['total_audit_hits'] = df['file_hits'] + df['net_hits'] + df['exec_hits']\n",
        "df['audit_detected'] = df['total_audit_hits'] > 0\n",
        "df['wazuh_detected'] = df['wazuh_alerts'] > 0\n",
        "df['iouring_detected'] = df['iouring_hits'] > 0\n",
        "df['ttd_seconds'] = df['time_to_detect'].replace(-1, np.nan)\n",
        "\n",
        "print('Cases:', df['case'].unique().tolist())\n",
        "print(f'Iterations: {df.iteration.nunique()}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ["## 3) Detection Rate Analysis"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "det_rates = df.groupby('case').agg({\n",
        "    'audit_detected': 'mean',\n",
        "    'iouring_detected': 'mean',\n",
        "    'wazuh_detected': 'mean',\n",
        "    'iteration': 'count'\n",
        "}).rename(columns={'iteration': 'N', 'audit_detected': 'Audit Rate', \n",
        "                   'iouring_detected': 'iouring Setup Rate', 'wazuh_detected': 'Wazuh Rate'})\n",
        "det_rates['Method'] = det_rates.index.map(lambda x: 'io_uring' if 'uring' in x else 'traditional')\n",
        "\n",
        "print('=== Detection Rates by Case ===')\n",
        "display_rates = det_rates.copy()\n",
        "for c in ['Audit Rate', 'iouring Setup Rate', 'Wazuh Rate']:\n",
        "    display_rates[c] = (display_rates[c] * 100).round(1).astype(str) + '%'\n",
        "print(display_rates.to_string())\n",
        "det_rates.to_csv(TABLES_DIR / 'detection_rates.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Summary by method\n",
        "method_summary = df.groupby('method')['audit_detected'].agg(['mean', 'sum', 'count'])\n",
        "method_summary.columns = ['Detection Rate', 'Detections', 'Total Runs']\n",
        "print('\\n=== Traditional vs io_uring ===')\n",
        "print(method_summary)\n",
        "\n",
        "if len(method_summary) == 2:\n",
        "    trad = method_summary.loc['traditional', 'Detection Rate']\n",
        "    uring = method_summary.loc['io_uring', 'Detection Rate']\n",
        "    print(f'\\n>>> EVASION EFFECTIVENESS: {(trad-uring)*100:.1f}% detection reduction')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ["## 4) False Negative Rate"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fn_rates = df.groupby('case')['audit_detected'].apply(lambda x: 1 - x.mean())\n",
        "fn_df = fn_rates.to_frame('False Negative Rate')\n",
        "fn_df['Method'] = fn_df.index.map(lambda x: 'io_uring' if 'uring' in x else 'traditional')\n",
        "print('=== False Negative Rates ===')\n",
        "print('(Higher = more evasion)\\n')\n",
        "fn_display = fn_df.copy()\n",
        "fn_display['False Negative Rate'] = (fn_display['False Negative Rate'] * 100).round(1).astype(str) + '%'\n",
        "print(fn_display.to_string())\n",
        "fn_df.to_csv(TABLES_DIR / 'false_negative_rates.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ["## 5) Time-to-Detection"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ttd = df[df['ttd_seconds'].notna()].groupby('case')['ttd_seconds'].agg(['count', 'median', 'mean', 'std'])\n",
        "ttd.columns = ['Detected', 'Median TTD', 'Mean TTD', 'Std']\n",
        "if len(ttd) > 0:\n",
        "    print('=== Time-to-Detection (seconds) ===')\n",
        "    print(ttd.round(3).to_string())\n",
        "    ttd.to_csv(TABLES_DIR / 'time_to_detection.csv')\n",
        "else:\n",
        "    print('No TTD data available (older CSV format or no detections)')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ["## 6) Syscall Bypass Validation Confirming io_uring truly bypasses syscall traces"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pairs = [\n",
        "    ('file_io_traditional', 'file_io_uring', 'file_hits', 'File Write'),\n",
        "    ('read_file_traditional', 'openat_uring', 'file_hits', 'File Read'),\n",
        "    ('net_connect_traditional', 'net_connect_uring', 'net_hits', 'Network'),\n",
        "]\n",
        "\n",
        "print('='*70)\n",
        "print('SYSCALL BYPASS VALIDATION')\n",
        "print('='*70)\n",
        "\n",
        "validation = []\n",
        "for trad_case, uring_case, metric, op in pairs:\n",
        "    trad = df[df['case'] == trad_case][metric]\n",
        "    uring = df[df['case'] == uring_case][metric]\n",
        "    if len(trad) == 0 or len(uring) == 0: continue\n",
        "    \n",
        "    t_mean, u_mean = trad.mean(), uring.mean()\n",
        "    if trad.std() == 0 and uring.std() == 0:\n",
        "        p = 0.0 if t_mean > u_mean else 1.0\n",
        "    else:\n",
        "        _, p = stats.mannwhitneyu(trad, uring, alternative='greater')\n",
        "    \n",
        "    bypass = p < 0.05 and t_mean > u_mean and u_mean < 1\n",
        "    validation.append({'Operation': op, 'Trad Mean': round(t_mean,2), \n",
        "                       'Uring Mean': round(u_mean,2), 'p-value': round(p,6),\n",
        "                       'Bypass Confirmed': 'YES' if bypass else 'NO'})\n",
        "    print(f'\\n{op}: Traditional={t_mean:.1f}, io_uring={u_mean:.1f}, p={p:.4f}')\n",
        "    print(f'  >>> Bypass Confirmed: {\"YES\" if bypass else \"NO\"}')\n",
        "\n",
        "val_df = pd.DataFrame(validation)\n",
        "val_df.to_csv(TABLES_DIR / 'syscall_bypass_validation.csv', index=False)\n",
        "print(f'\\nSaved: {TABLES_DIR}/syscall_bypass_validation.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ["## 7) Visualizations"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Figure 1: Detection Rate Bar Chart\n",
        "fig, ax = plt.subplots(figsize=(12, 6))\n",
        "rates = df.groupby('case')['audit_detected'].mean() * 100\n",
        "colors = ['#2ecc71' if 'traditional' in c else '#e74c3c' for c in rates.index]\n",
        "bars = ax.bar(range(len(rates)), rates.values, color=colors, edgecolor='black')\n",
        "ax.set_xticks(range(len(rates)))\n",
        "ax.set_xticklabels(rates.index, rotation=45, ha='right')\n",
        "ax.set_ylabel('Detection Rate (%)')\n",
        "ax.set_title('Figure 1: Audit Detection Rate by Test Case', fontweight='bold')\n",
        "ax.set_ylim(0, 110)\n",
        "for bar, val in zip(bars, rates.values):\n",
        "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 2, f'{val:.0f}%', ha='center', fontweight='bold')\n",
        "from matplotlib.patches import Patch\n",
        "ax.legend(handles=[Patch(color='#2ecc71', label='Traditional'), Patch(color='#e74c3c', label='io_uring')])\n",
        "plt.tight_layout()\n",
        "plt.savefig(FIGURES_DIR / 'fig1_detection_rates.png', dpi=300)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Figure 2: Paired Comparison\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "labels = ['File Write', 'File Read', 'Network']\n",
        "trad_cases = ['file_io_traditional', 'read_file_traditional', 'net_connect_traditional']\n",
        "uring_cases = ['file_io_uring', 'openat_uring', 'net_connect_uring']\n",
        "t_means = [df[df['case']==c]['total_audit_hits'].mean() for c in trad_cases]\n",
        "u_means = [df[df['case']==c]['total_audit_hits'].mean() for c in uring_cases]\n",
        "x = np.arange(len(labels))\n",
        "ax.bar(x - 0.175, t_means, 0.35, label='Traditional', color='#2ecc71', edgecolor='black')\n",
        "ax.bar(x + 0.175, u_means, 0.35, label='io_uring', color='#e74c3c', edgecolor='black')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(labels)\n",
        "ax.set_ylabel('Mean Audit Events')\n",
        "ax.set_title('Figure 2: Mean Audit Events - Traditional vs io_uring', fontweight='bold')\n",
        "ax.legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig(FIGURES_DIR / 'fig2_paired_comparison.png', dpi=300)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Figure 3: Box Plots\n",
        "fig, axes = plt.subplots(1, 3, figsize=(14, 5))\n",
        "for ax, (t, u, m, title) in zip(axes, [\n",
        "    ('file_io_traditional', 'file_io_uring', 'file_hits', 'File Write'),\n",
        "    ('read_file_traditional', 'openat_uring', 'file_hits', 'File Read'),\n",
        "    ('net_connect_traditional', 'net_connect_uring', 'net_hits', 'Network')]):\n",
        "    bp = ax.boxplot([df[df['case']==t][m], df[df['case']==u][m]], \n",
        "                    labels=['Traditional', 'io_uring'], patch_artist=True)\n",
        "    bp['boxes'][0].set_facecolor('#2ecc71')\n",
        "    bp['boxes'][1].set_facecolor('#e74c3c')\n",
        "    ax.set_title(title, fontweight='bold')\n",
        "    ax.set_ylabel('Audit Events')\n",
        "fig.suptitle('Figure 3: Distribution of Audit Events', fontweight='bold', y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.savefig(FIGURES_DIR / 'fig3_boxplots.png', dpi=300)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Figure 4: Heatmap\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "hm_data = df.groupby('case')[['file_hits', 'net_hits', 'exec_hits', 'iouring_hits']].apply(lambda x: (x > 0).mean()) * 100\n",
        "hm_data.columns = ['File', 'Network', 'Exec', 'io_uring Setup']\n",
        "hm_data = hm_data.reindex(sorted(hm_data.index, key=lambda x: (0 if 'traditional' in x else 1, x)))\n",
        "sns.heatmap(hm_data, annot=True, fmt='.0f', cmap='RdYlGn', vmin=0, vmax=100, ax=ax, cbar_kws={'label': 'Detection %'})\n",
        "ax.set_title('Figure 4: Detection Heatmap', fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.savefig(FIGURES_DIR / 'fig4_heatmap.png', dpi=300)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Figure 5: Evasion Effectiveness\n",
        "fig, ax = plt.subplots(figsize=(8, 5))\n",
        "ops = ['File Write', 'File Read', 'Network']\n",
        "evasion = []\n",
        "for t, u in [('file_io_traditional', 'file_io_uring'), ('read_file_traditional', 'openat_uring'), ('net_connect_traditional', 'net_connect_uring')]:\n",
        "    tr = df[df['case']==t]['audit_detected'].mean() * 100\n",
        "    ur = df[df['case']==u]['audit_detected'].mean() * 100\n",
        "    evasion.append(tr - ur)\n",
        "colors = ['#3498db' if e > 50 else '#f39c12' if e > 0 else '#95a5a6' for e in evasion]\n",
        "ax.barh(ops, evasion, color=colors, edgecolor='black')\n",
        "for i, v in enumerate(evasion): ax.text(v + 2, i, f'{v:.0f}%', va='center', fontweight='bold')\n",
        "ax.set_xlabel('Detection Rate Reduction (%)')\n",
        "ax.set_title('Figure 5: io_uring Evasion Effectiveness', fontweight='bold')\n",
        "ax.axvline(0, color='black')\n",
        "plt.tight_layout()\n",
        "plt.savefig(FIGURES_DIR / 'fig5_evasion.png', dpi=300)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ["## 8) MITRE ATT&CK Mapping"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mitre = pd.DataFrame([\n",
        "    {'Technique': 'T1059', 'Name': 'Command and Scripting Interpreter', 'Test Case': 'exec_cmd_traditional', 'io_uring Evasion': 'N/A (execve not supported)'},\n",
        "    {'Technique': 'T1005', 'Name': 'Data from Local System', 'Test Case': 'file_io_*, read_file_*, openat_*', 'io_uring Evasion': 'Yes'},\n",
        "    {'Technique': 'T1071', 'Name': 'Application Layer Protocol', 'Test Case': 'net_connect_*', 'io_uring Evasion': 'Yes'},\n",
        "    {'Technique': 'T1562.001', 'Name': 'Impair Defenses: Disable/Modify Tools', 'Test Case': 'All io_uring variants', 'io_uring Evasion': 'Primary finding'},\n",
        "])\n",
        "print('=== MITRE ATT&CK Mapping ===')\n",
        "print(mitre.to_string(index=False))\n",
        "mitre.to_csv(TABLES_DIR / 'mitre_mapping.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ["## 9) Executive Summary"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('='*70)\n",
        "print('EXECUTIVE SUMMARY')\n",
        "print('='*70)\n",
        "print(f'Total runs: {len(df)}')\n",
        "print(f'Iterations: {df.iteration.nunique()}')\n",
        "trad_df = df[df['method'] == 'traditional']\n",
        "uring_df = df[df['method'] == 'io_uring']\n",
        "print(f'\\nTraditional detection rate: {trad_df.audit_detected.mean():.1%}')\n",
        "print(f'io_uring detection rate: {uring_df.audit_detected.mean():.1%}')\n",
        "print(f'\\nEvasion effectiveness: {(trad_df.audit_detected.mean() - uring_df.audit_detected.mean()):.1%}')\n",
        "print('\\nConclusion: io_uring operations successfully bypass syscall-based')\n",
        "print('monitoring. Defenders should implement eBPF-based detection or')\n",
        "print('monitor io_uring_setup/io_uring_enter syscalls as behavioral indicators.')\n",
        "print('='*70)\n",
        "print(f'\\nOutput files saved to: {RESULTS_DIR}/')\n",
        "print('Figures:', list(FIGURES_DIR.glob('*.png')))\n",
        "print('Tables:', list(TABLES_DIR.glob('*.csv')))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"},
    "language_info": {"name": "python", "version": "3.10"}
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
